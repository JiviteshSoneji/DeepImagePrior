{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "# import cv2\n",
    "\n",
    "\n",
    "def manual_vertical_streaks(image, streak_intensity=255, streak_width=1, horizontal_spacing=5):\n",
    "    \"\"\"\n",
    "    Manually add vertical streaks to an image.\n",
    "\n",
    "    Parameters:\n",
    "    image (PIL.Image): Image to add streaks to.\n",
    "    streak_intensity (int): The intensity of the streaks, 255 for white.\n",
    "    streak_width (int): The width of the streaks.\n",
    "    horizontal_spacing (int): The horizontal spacing between streaks.\n",
    "\n",
    "    Returns:\n",
    "    PIL.Image: The image with added vertical streaks.\n",
    "    \"\"\"\n",
    "    # Convert image to a numpy array\n",
    "    img_array = np.array(image)\n",
    "    # Get the dimensions of the image\n",
    "    height, width,c = img_array.shape\n",
    "\n",
    "    # We'll add vertical streaks at regular intervals\n",
    "    for x in range(0, width, horizontal_spacing):\n",
    "        # Add a vertical streak down the height of the image\n",
    "        if x + streak_width < width:  # Check if the streak is within the image bounds\n",
    "            img_array[:, x:x + streak_width] = streak_intensity\n",
    "\n",
    "    # Convert the numpy array back to a PIL image\n",
    "    return Image.fromarray(img_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "fnames = glob.glob('/Users/mihiragarwal/Desktop/Project Courses/Muse Lab Work/DeepImagePrior/cropped_images_e926c20632_without_streaks/*.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mihiragarwal/Desktop/Project Courses/Muse Lab Work/DeepImagePrior/cropped_images_e926c20632_without_streaks/new_cropped_ezgif-frame-005.jpg'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n",
      "(1345, 1516, 3)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for i in fnames:\n",
    "    img = Image.open(i)\n",
    "    img = manual_vertical_streaks(img, streak_intensity=255,\n",
    "    streak_width=3,  # Increased width of the streaks\n",
    "    horizontal_spacing=40 )\n",
    "    match = re.search(r'(\\d+).jpg', i)\n",
    "\n",
    "# Retrieve the matched group if it exists\n",
    "    number = match.group(1) if match else None\n",
    "    img.save(f\"/Users/mihiragarwal/Desktop/Project Courses/Muse Lab Work/DeepImagePrior/images_with_streak/image_{number}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_without_streaks = Image.open(\"/Users/mihiragarwal/Desktop/Project Courses/Muse Lab Work/DeepImagePrior/cropped_images_e926c20632_without_streaks/new_cropped_ezgif-frame-001.jpg\")   \n",
    "# save image without streaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply manual vertical streaks to the image without streaks\n",
    "image_with_fewer_wider_streaks = manual_vertical_streaks(\n",
    "    image_without_streaks.convert('L'),\n",
    "    streak_intensity=255,\n",
    "    streak_width=3,  # Increased width of the streaks\n",
    "    horizontal_spacing=40  # Increased spacing between the streaks\n",
    ")\n",
    "\n",
    "# Save the result\n",
    "\n",
    "\n",
    "# Show the result\n",
    "image_with_fewer_wider_streaks.show()\n",
    "\n",
    "# Return the path to the saved image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectNetworkSmall(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DirectNetworkSmall, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),  # Small model with fewer filters\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1),  # One hidden layer\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=16, out_channels=3, kernel_size=3, padding=1),  # Output 3 channels for RGB\n",
    "            nn.BatchNorm2d(3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small_rgb = DirectNetworkSmall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DirectNetworkSmall(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_small_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Compose from torchvision.transforms\n",
    "from torchvision.transforms import Compose\n",
    "# import ToTensor from torchvision.transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "# import Normalize from torchvision.transforms\n",
    "from torchvision.transforms import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_rgb = Compose([ToTensor()])\n",
    "input_rgb_tensor = transform_rgb(image_with_fewer_wider_streaks.convert('RGB')).unsqueeze(0)\n",
    "ina = transform_rgb(image_without_streaks.convert('RGB')).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    trained_output_tensor = model_small_rgb(input_rgb_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = transforms.ToPILImage()(input_rgb_tensor.squeeze(0))\n",
    "input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the output tensor to an image\n",
    "# import transforms from torchvision    \n",
    "from torchvision import transforms\n",
    "output_image = transforms.ToPILImage()(trained_output_tensor.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectNetworkRGB(nn.Module):\n",
    "    def __init__(self, num_layers=5):\n",
    "        super(DirectNetworkRGB, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Define the first layer which accepts the input RGB image\n",
    "        self.layers.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),  # Small model with fewer filters\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ))\n",
    "        \n",
    "        # Add subsequent layers, each receiving the output of the previous layer\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1),  # Small model with fewer filters\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "        \n",
    "        # Define the last layer which outputs the RGB image\n",
    "        self.layers.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=3, kernel_size=3, padding=1),  # Output 3 channels for RGB\n",
    "            nn.BatchNorm2d(3)\n",
    "        ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through each layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the network for RGB images\n",
    "model_rgb = DirectNetworkRGB(num_layers=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_rgb.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.183054804801941\n",
      "Epoch [2/10], Loss: 0.9671365022659302\n",
      "Epoch [3/10], Loss: 0.8728942275047302\n",
      "Epoch [4/10], Loss: 0.8051985502243042\n",
      "Epoch [5/10], Loss: 0.7451328635215759\n",
      "Epoch [6/10], Loss: 0.6934181451797485\n",
      "Epoch [7/10], Loss: 0.6582513451576233\n",
      "Epoch [8/10], Loss: 0.6357120871543884\n",
      "Epoch [9/10], Loss: 0.6180820465087891\n",
      "Epoch [10/10], Loss: 0.6041207909584045\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_rgb(input_rgb_tensor)\n",
    "    loss = criterion(outputs, input_rgb_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "# After training, run the model on the input tensor\n",
    "with torch.no_grad():\n",
    "    trained_output_tensor = model_rgb(input_rgb_tensor)\n",
    "\n",
    "# Convert the trained output tensor back to an image for display\n",
    "trained_output_image = trained_output_tensor.squeeze().cpu().detach().numpy()\n",
    "# trained_output_image = (trained_output_image * 0.5 + 0.5) * 255  # Denormalize and scale back to 0-255 range\n",
    "trained_output_image = trained_output_image.transpose(1, 2, 0).astype(np.uint8)  # Change to HxWxC format\n",
    "trained_output_pil_image = Image.fromarray(trained_output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_output_pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F_nn\n",
    "\n",
    "# Then replace F.conv2d with F_nn.conv2d within your GuidedFilter class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class GuidedFilter(nn.Module):\n",
    "    def __init__(self, radius=15, eps=1e-10):\n",
    "        super(GuidedFilter, self).__init__()\n",
    "        self.radius = radius\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, guidance, detail):\n",
    "        b, c, h, w = guidance.size()\n",
    "        pad = self.radius\n",
    "        I = F.pad(guidance, (pad, pad, pad, pad))\n",
    "        p = F.pad(detail, (pad, pad, pad, pad))\n",
    "        ones = torch.ones((b, c, h + 2 * pad, w + 2 * pad), dtype=guidance.dtype, device=guidance.device)\n",
    "        N = F_nn.conv2d(ones, torch.ones((b, c, 2 * self.radius + 1, 2 * self.radius + 1), device=guidance.device), groups=c, padding=self.radius)\n",
    "        mean_I = F_nn.conv2d(I, torch.ones((b, 1, 2 * self.radius + 1, 2 * self.radius + 1), device=guidance.device), groups=c, padding=self.radius) / N\n",
    "        mean_p = F_nn.conv2d(p, torch.ones((1, 1, 2 * self.radius + 1, 2 * self.radius + 1), device=guidance.device), padding=self.radius) / N\n",
    "        mean_Ip = F_nn.conv2d(I * p, ones, stride=1, padding=pad) / N\n",
    "        cov_Ip = mean_Ip - mean_I * mean_p\n",
    "        mean_II = F_nn.conv2d(I * I, ones, stride=1, padding=pad) / N\n",
    "        var_I = mean_II - mean_I * mean_I\n",
    "        a = cov_Ip / (var_I + self.eps)\n",
    "        b = mean_p - a * mean_I\n",
    "        mean_a = F_nn.conv2d(a, ones, stride=1, padding=pad) / N\n",
    "        mean_b = F_nn.conv2d(b, ones, stride=1, padding=pad) / N\n",
    "        q = mean_a * I + mean_b\n",
    "        return q\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        return out + x\n",
    "\n",
    "class DerainingNetwork(nn.Module):\n",
    "    def __init__(self, num_channels=3, num_blocks=2):\n",
    "        super(DerainingNetwork, self).__init__()\n",
    "        self.guided_filter = GuidedFilter()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 16, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.res_blocks = nn.ModuleList([ResidualBlock(16, 16) for _ in range(num_blocks)])\n",
    "        self.conv2 = nn.Conv2d(16, num_channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # detail = x - self.guided_filter(x, x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        for block in self.res_blocks:\n",
    "            out = block(out)\n",
    "        out = self.conv2(out)\n",
    "        return x - out\n",
    "\n",
    "model = DerainingNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=3, expected weight to be at least 3 at dimension 0, but got weight of size [1, 3, 31, 31] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/mihiragarwal/Desktop/Project Courses/Muse Lab Work/DeepImagePrior/adding_streaks.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mihiragarwal/Desktop/Project%20Courses/Muse%20Lab%20Work/DeepImagePrior/adding_streaks.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfilter\u001b[39m \u001b[39m=\u001b[39m GuidedFilter()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mihiragarwal/Desktop/Project%20Courses/Muse%20Lab%20Work/DeepImagePrior/adding_streaks.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfilter\u001b[39;49m(input_rgb_tensor, input_rgb_tensor)\n",
      "File \u001b[0;32m~/mambaforge/envs/chem/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/mihiragarwal/Desktop/Project Courses/Muse Lab Work/DeepImagePrior/adding_streaks.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihiragarwal/Desktop/Project%20Courses/Muse%20Lab%20Work/DeepImagePrior/adding_streaks.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m p \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpad(detail, (pad, pad, pad, pad))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihiragarwal/Desktop/Project%20Courses/Muse%20Lab%20Work/DeepImagePrior/adding_streaks.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m ones \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((b, c, h \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m pad, w \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m pad), dtype\u001b[39m=\u001b[39mguidance\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mguidance\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mihiragarwal/Desktop/Project%20Courses/Muse%20Lab%20Work/DeepImagePrior/adding_streaks.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m N \u001b[39m=\u001b[39m F_nn\u001b[39m.\u001b[39;49mconv2d(ones, torch\u001b[39m.\u001b[39;49mones((b, c, \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mradius \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mradius \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), device\u001b[39m=\u001b[39;49mguidance\u001b[39m.\u001b[39;49mdevice), groups\u001b[39m=\u001b[39;49mc, padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mradius)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihiragarwal/Desktop/Project%20Courses/Muse%20Lab%20Work/DeepImagePrior/adding_streaks.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m mean_I \u001b[39m=\u001b[39m F_nn\u001b[39m.\u001b[39mconv2d(I, torch\u001b[39m.\u001b[39mones((b, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mradius \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mradius \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), device\u001b[39m=\u001b[39mguidance\u001b[39m.\u001b[39mdevice), groups\u001b[39m=\u001b[39mc, padding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mradius) \u001b[39m/\u001b[39m N\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihiragarwal/Desktop/Project%20Courses/Muse%20Lab%20Work/DeepImagePrior/adding_streaks.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m mean_p \u001b[39m=\u001b[39m F_nn\u001b[39m.\u001b[39mconv2d(p, torch\u001b[39m.\u001b[39mones((\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mradius \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mradius \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), device\u001b[39m=\u001b[39mguidance\u001b[39m.\u001b[39mdevice), padding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mradius) \u001b[39m/\u001b[39m N\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=3, expected weight to be at least 3 at dimension 0, but got weight of size [1, 3, 31, 31] instead"
     ]
    }
   ],
   "source": [
    "filter = GuidedFilter()\n",
    "filter(input_rgb_tensor, input_rgb_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.ToPILImage()(model(input_rgb_tensor).squeeze(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7225360870361328\n",
      "Epoch [2/10], Loss: 0.7225360870361328\n",
      "Epoch [3/10], Loss: 0.7225360870361328\n",
      "Epoch [4/10], Loss: 0.7225360870361328\n",
      "Epoch [5/10], Loss: 0.7225360870361328\n",
      "Epoch [6/10], Loss: 0.7225360870361328\n",
      "Epoch [7/10], Loss: 0.7225360870361328\n",
      "Epoch [8/10], Loss: 0.7225360870361328\n",
      "Epoch [9/10], Loss: 0.7225360870361328\n",
      "Epoch [10/10], Loss: 0.7225360870361328\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_rgb_tensor)\n",
    "    loss = criterion(outputs, ina)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "# After training, run the model on the input tensor\n",
    "with torch.no_grad():\n",
    "    trained_output_tensor = model_rgb(input_rgb_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.ToPILImage()(model(trained_output_tensor).squeeze(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
